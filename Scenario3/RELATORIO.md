# RelatÃ³rio de AnÃ¡lise de Desempenho - Scenario 3: Teste de Stress

## InformaÃ§Ãµes do Teste

**Data de ExecuÃ§Ã£o:** 7 de Dezembro de 2025  
**DuraÃ§Ã£o:** 120 segundos  
**UsuÃ¡rios SimultÃ¢neos:** 100 (aumento de 100% vs Scenarios 1 e 2)  
**DistribuiÃ§Ã£o de Carga:** Ramp-up linear (0-100 usuÃ¡rios em 30 segundos)  
**Range de Entrada:** NÃºmeros de 20 a 70  
**Endpoints Testados:** 2 (/api/fib/ e /api/fib-lote/)  
**ConfiguraÃ§Ã£o de Infraestrutura:** HPA habilitado (2-8 rÃ©plicas por serviÃ§o)

---

## Resumo Executivo

O teste de stress Scenario 3 revelou **degradaÃ§Ã£o crÃ­tica do sistema** sob carga extrema. Das 6.763 requisiÃ§Ãµes processadas, **4.271 falharam com erro HTTP 500** (Internal Server Error), resultando em **taxa de falha de 63,14%**. O sistema demonstrou incapacidade de manter operaÃ§Ã£o estÃ¡vel com 100 usuÃ¡rios simultÃ¢neos, apresentando saturaÃ§Ã£o de recursos e colapso de performance apÃ³s 60 segundos de teste. A latÃªncia mediana atingiu 480ms (aumento de 4.700% comparado ao baseline) e percentil P95 alcanÃ§ou 760ms, com casos extremos ultrapassando 1.400ms.

### MÃ©tricas Principais

| MÃ©trica | Resultado | Status |
|---------|-----------|--------|
| Total de RequisiÃ§Ãµes | 6.763 | Alta carga processada |
| **Taxa de Falha** | **63,14%** (4.271 falhas) | âŒ CRÃTICO |
| RequisiÃ§Ãµes Bem-Sucedidas | 2.492 (36,86%) | âŒ InaceitÃ¡vel |
| Throughput MÃ©dio | 56,69 req/s | âš ï¸ Abaixo da capacidade nominal |
| **LatÃªncia P50** | **480 ms** | âŒ DegradaÃ§Ã£o severa |
| **LatÃªncia P95** | **760 ms** | âŒ ExperiÃªncia ruim |
| **LatÃªncia P99** | **890 ms** | âŒ Tail latency extrema |
| LatÃªncia MÃ¡xima | 1.449,31 ms | âŒ Timeout crÃ­tico |

**ClassificaÃ§Ã£o de SaÃºde:** âŒ **CRÃTICO - Sistema nÃ£o suporta carga de 100 usuÃ¡rios**

---

## AnÃ¡lise de Desempenho por Endpoint

### POST /api/fib/ - RequisiÃ§Ãµes de NÃºmero Ãšnico

Este endpoint processou a maioria do trÃ¡fego (91,67% das requisiÃ§Ãµes totais) e apresentou degradaÃ§Ã£o severa sob stress.

| MÃ©trica | Resultado |
|---------|-----------|
| RequisiÃ§Ãµes Totais | 6.200 |
| **Falhas** | **3.708 (59,81%)** | âŒ |
| RequisiÃ§Ãµes Bem-Sucedidas | 2.492 (40,19%) |
| Throughput | 51,97 req/s |
| LatÃªncia P50 | 480 ms |
| LatÃªncia P95 | 760 ms |
| LatÃªncia P99 | 880 ms |
| LatÃªncia MÃ¡xima | 1.420,18 ms |

**AnÃ¡lise:** O endpoint principal colapsou sob stress, com quase 60% das requisiÃ§Ãµes resultando em HTTP 500. A latÃªncia mediana de 480ms representa degradaÃ§Ã£o de 4.700% comparada ao baseline (10ms). Isso indica saturaÃ§Ã£o de CPU, esgotamento de threads ou timeout de comunicaÃ§Ã£o gRPC.

### POST /api/fib-lote/ - RequisiÃ§Ãµes em Lote

Endpoint de processamento batch apresentou **colapso total** com 100% de taxa de falha.

| MÃ©trica | Resultado |
|---------|-----------|
| RequisiÃ§Ãµes Totais | 563 |
| **Falhas** | **563 (100%)** | âŒ |
| RequisiÃ§Ãµes Bem-Sucedidas | 0 (0%) |
| Throughput | 4,72 req/s |
| LatÃªncia P50 | 510 ms |
| LatÃªncia P95 | 790 ms |
| LatÃªncia P99 | 920 ms |
| LatÃªncia MÃ¡xima | 1.449,31 ms |

**AnÃ¡lise CrÃ­tica:** O endpoint de lote apresentou **100% de taxa de falha**, indicando que operaÃ§Ãµes complexas (mÃºltiplos cÃ¡lculos Fibonacci por requisiÃ§Ã£o) excedem completamente a capacidade do sistema sob carga extrema. Isso sugere que o backend gRPC (Java ou Node.js) estÃ¡ travando ou rejeitando conexÃµes por sobrecarga.

---

## Perfil de Carga e Comportamento Temporal

### Fase 1: Ramp-up (0-30 segundos, 0-100 usuÃ¡rios)

Durante a fase inicial, o sistema ainda apresentava relativa estabilidade, mas com sinais precoces de degradaÃ§Ã£o.

**Comportamento observado:**
- Segundos 0-10 (5-15 usuÃ¡rios): Taxa de falha inicial de ~50-60%, latÃªncia P50 ~40-50ms
- Segundos 10-20 (20-45 usuÃ¡rios): Taxa de falha aumenta para ~60-70%, latÃªncia P50 sobe para 30-70ms com spikes atÃ© 1.400ms
- Segundos 20-30 (50-100 usuÃ¡rios): Sistema entra em colapso progressivo, falhas atingem 70-80%, latÃªncia P50 chega a 82-130ms

**InterpretaÃ§Ã£o:** O sistema comeÃ§ou a falhar imediatamente apÃ³s atingir 10-15 usuÃ¡rios, indicando que o limite de capacidade estÃ¡ significativamente abaixo de 100 usuÃ¡rios simultÃ¢neos.

### Fase 2: Pico Sustentado (30-90 segundos, 100 usuÃ¡rios constantes)

PerÃ­odo de maior degradaÃ§Ã£o, onde o sistema opera em modo de colapso contÃ­nuo.

**Comportamento observado:**
- Throughput nominal: 60-64 req/s (mas com 60-65% de falhas)
- LatÃªncia P50: aumenta progressivamente de 100ms (30s) para 440ms (90s)
- LatÃªncia P95: cresce de 480ms para 740ms
- Taxa de falha: mantÃ©m-se entre 60-65% durante todo o perÃ­odo

**InterpretaÃ§Ã£o crÃ­tica:** O sistema entra em estado de **saturaÃ§Ã£o permanente**. A latÃªncia cresce linearmente ao longo do tempo (de 100ms para 440ms em 60 segundos), sugerindo acÃºmulo de backlog, exaustÃ£o de pool de threads ou memory pressure crescente. A taxa de falha consistente indica que aproximadamente 2/3 das requisiÃ§Ãµes sÃ£o rejeitadas ou excedem timeout.

### Fase 3: SustentaÃ§Ã£o Prolongada (90-120 segundos, 100 usuÃ¡rios)

Fase final do teste, onde o sistema se estabiliza em estado degradado permanente.

**Comportamento observado:**
- LatÃªncia P50 estabiliza em 450-480ms
- LatÃªncia P95 mantÃ©m-se em 730-770ms
- Taxa de falha reduz levemente para 55-60%
- Throughput cai para 52-60 req/s

**InterpretaÃ§Ã£o:** O sistema atinge um "novo equilÃ­brio" degradado, onde aproximadamente metade das requisiÃ§Ãµes sÃ£o bem-sucedidas com latÃªncia 40-50x superior ao baseline. Isso sugere que o autoscaling (HPA) pode ter criado novas rÃ©plicas, aliviando parcialmente a pressÃ£o, mas sem resolver o problema fundamental de capacidade.

---

## DistribuiÃ§Ã£o de LatÃªncia

A distribuiÃ§Ã£o de latÃªncia no Scenario 3 apresenta valores extremamente elevados em todos os percentis, indicando experiÃªncia de usuÃ¡rio inaceitÃ¡vel.

| Percentil | LatÃªncia | InterpretaÃ§Ã£o |
|-----------|----------|----------------|
| P50 | 480 ms | 50% das requisiÃ§Ãµes levam quase meio segundo |
| P66 | 550 ms | 2/3 das requisiÃ§Ãµes excedem 500ms |
| P75 | 600 ms | 3/4 das requisiÃ§Ãµes ultrapassam 600ms |
| P80 | 630 ms | 80% acima de 600ms |
| P90 | 710 ms | 90% acima de 700ms |
| **P95** | **760 ms** | Apenas 5% completam em < 760ms |
| P99 | 890 ms | 1% das requisiÃ§Ãµes leva quase 1 segundo |
| P99.9 | 1.300 ms | 0.1% excede 1,3 segundos |
| P100 (Max) | 1.449,31 ms | Casos extremos prÃ³ximos a timeout |

**AnÃ¡lise de SLA Compliance:**

âŒ **P95 < 100ms:** FALHA (resultado: 760ms, 660% acima do limite)  
âŒ **P99 < 200ms:** FALHA (resultado: 890ms, 345% acima do limite)  
âŒ **MÃ¡ximo < 2.000ms:** APROVADO por margem mÃ­nima (resultado: 1.449ms)

**ConclusÃ£o:** O sistema viola completamente os Service Level Agreements esperados para aplicaÃ§Ãµes interativas. LatÃªncias acima de 500ms resultam em experiÃªncia de usuÃ¡rio severamente degradada e inaceitÃ¡vel para produÃ§Ã£o.

---

## ComparaÃ§Ã£o com Scenarios Anteriores

A comparaÃ§Ã£o entre os trÃªs scenarios revela claramente o ponto de saturaÃ§Ã£o do sistema.

| MÃ©trica | Scenario 1 (Baseline) | Scenario 2 (Alta Carga) | Scenario 3 (Stress) | Î” S3 vs S1 |
|---------|----------------------|------------------------|---------------------|-----------|
| UsuÃ¡rios | 50 | 50 | 100 | +100% |
| Entrada | 5-35 | 20-70 | 20-70 | = |
| HPA | 1 rÃ©plica fixa | 1 rÃ©plica fixa | 2-8 rÃ©plicas | Autoscaling |
| **Total RequisiÃ§Ãµes** | 3.540 | 4.852 | 6.763 | +91% |
| **Taxa de Falha** | **0%** | **0%** | **63,14%** | âŒ +63,14% |
| **Taxa de Sucesso** | **100%** | **100%** | **36,86%** | âŒ -63,14% |
| Throughput | 29,7 req/s | 40,72 req/s | 56,69 req/s | +91% |
| **LatÃªncia P50** | 10 ms | 10 ms | **480 ms** | âŒ +4.700% |
| **LatÃªncia P95** | 24 ms | 39 ms | **760 ms** | âŒ +3.067% |
| **LatÃªncia P99** | 50 ms | 60 ms | **890 ms** | âŒ +1.680% |
| LatÃªncia MÃ¡xima | 140 ms | 153,90 ms | 1.449,31 ms | âŒ +935% |

**InterpretaÃ§Ã£o Comparativa:**

O dobro de usuÃ¡rios (50 â†’ 100) resultou em colapso catastrÃ³fico do sistema. Enquanto Scenarios 1 e 2 mantiveram 100% de sucesso com latÃªncias aceitÃ¡veis (10-60ms P99), o Scenario 3 apresentou degradaÃ§Ã£o de mÃºltiplas ordens de magnitude. A latÃªncia P50 aumentou 48 vezes, e a taxa de sucesso caiu de 100% para 37%. 

Isso indica que o **limite de capacidade estÃ¡ entre 50 e 100 usuÃ¡rios**, provavelmente em torno de 60-70 usuÃ¡rios simultÃ¢neos. O autoscaling (HPA) foi insuficiente para compensar a carga, sugerindo bottleneck em recursos nÃ£o escalÃ¡veis (CPU por pod, timeout de gRPC, pool de conexÃµes) ou tempo de inicializaÃ§Ã£o de novas rÃ©plicas superior Ã  velocidade de aumento de carga.

---

## AnÃ¡lise de Falhas

### Tipos de Erro Registrados

| Tipo de Erro | OcorrÃªncias | Percentual do Total |
|--------------|-------------|---------------------|
| **HTTP 500 (Internal Server Error)** | **4.271** | **63,14%** |
| RequisiÃ§Ãµes Bem-Sucedidas | 2.492 | 36,86% |
| HTTP 4xx | 0 | 0% |
| Timeout | NÃ£o registrado | N/A |
| ExceÃ§Ãµes nÃ£o tratadas | 0 | 0% |
| ConexÃ£o Recusada | NÃ£o registrado | N/A |

### DistribuiÃ§Ã£o de Falhas por Endpoint

| Endpoint | Total | Falhas | Taxa de Falha |
|----------|-------|--------|---------------|
| /api/fib/ | 6.200 | 3.708 | 59,81% |
| /api/fib-lote/ | 563 | 563 | **100%** |

**AnÃ¡lise de Causa Raiz:**

O erro **HTTP 500 (Internal Server Error)** dominante indica falha no processamento backend, nÃ£o no cliente ou rede. As causas provÃ¡veis incluem:

1. **SaturaÃ§Ã£o de CPU:** Pods atingem 100% de uso e rejeitam novas requisiÃ§Ãµes
2. **Timeout de gRPC:** ComunicaÃ§Ã£o Django â†’ Node.js â†’ Java excede limite de tempo (default geralmente 30-60s)
3. **ExaustÃ£o de Thread Pool:** Backend Java ou Node.js sem threads disponÃ­veis para processar
4. **Memory Pressure:** Sistema entra em GC (Garbage Collection) excessivo ou OOM (Out of Memory)
5. **Cascading Failure:** Falha em um serviÃ§o (ex: Java) propaga para toda a cadeia

A **taxa de 100% de falha em /api/fib-lote/** Ã© particularmente reveladora: operaÃ§Ãµes que exigem mÃºltiplos cÃ¡lculos Fibonacci (nÃºmeros 20-70) sÃ£o computacionalmente caras demais para o sistema atual sob carga extrema.

---

## AnÃ¡lise de Recursos (HPA e Escalabilidade)

### Comportamento do Horizontal Pod Autoscaler

O teste foi executado com HPA habilitado (minReplicas: 2, maxReplicas: 8, target CPU: 70%). Apesar do autoscaling, o sistema falhou.

**HipÃ³teses sobre comportamento do HPA:**

1. **Tempo de InicializaÃ§Ã£o (Cold Start):** Novos pods levam 30-60s para estar prontos (pull de imagem, health checks, warmup). Durante ramp-up de 30s (0â†’100 usuÃ¡rios), novos pods nÃ£o tiveram tempo de inicializar.

2. **Limite de CPU atingido:** Se todos os 8 pods atingiram 100% de CPU simultaneamente, nÃ£o hÃ¡ mais capacidade disponÃ­vel para escalar.

3. **Bottleneck nÃ£o escalÃ¡vel:** O gargalo pode estar em recurso compartilhado (ex: banco de dados, se houver) ou em limitaÃ§Ã£o algorÃ­tmica (cÃ¡lculo Fibonacci recursivo sem memoizaÃ§Ã£o).

4. **Threshold de CPU:** Com target de 70%, HPA pode ter escalado tarde demais (quando CPU jÃ¡ estava em 90-100%).

**RecomendaÃ§Ã£o:** Coletar mÃ©tricas de CPU/MemÃ³ria dos pods durante o teste para confirmar se HPA escalou e quantos pods foram criados.

---

## ConclusÃµes e RecomendaÃ§Ãµes

### Status de SaÃºde do Sistema

**ClassificaÃ§Ã£o: âŒ CRÃTICO - NÃƒO APROVADO PARA PRODUÃ‡ÃƒO**

O sistema apresentou colapso operacional sob carga de 100 usuÃ¡rios simultÃ¢neos. Com taxa de falha de 63% e latÃªncias de 480-890ms (P50-P99), a experiÃªncia de usuÃ¡rio Ã© inaceitÃ¡vel e o sistema nÃ£o atende requisitos mÃ­nimos de disponibilidade.

### Capacidade MÃ¡xima Identificada

Baseado nos resultados dos trÃªs scenarios:

| CenÃ¡rio | UsuÃ¡rios | Taxa de Sucesso | LatÃªncia P95 | Status |
|---------|----------|----------------|--------------|--------|
| Scenario 1 | 50 | 100% | 24 ms | âœ… Excelente |
| Scenario 2 | 50 | 100% | 39 ms | âœ… Bom |
| Scenario 3 | 100 | 36,86% | 760 ms | âŒ CrÃ­tico |

**Capacidade Estimada:** O sistema suporta de forma confiÃ¡vel **atÃ© 50 usuÃ¡rios simultÃ¢neos**. Entre 50 e 100 usuÃ¡rios, hÃ¡ um ponto de quebra onde o sistema colapsa. Recomenda-se teste incremental (60, 70, 80 usuÃ¡rios) para identificar o limite exato.

### AÃ§Ãµes Imediatas (Bloqueantes para ProduÃ§Ã£o)

1. **OtimizaÃ§Ã£o de Algoritmo Fibonacci**
   - **Problema:** CÃ¡lculo recursivo sem memoizaÃ§Ã£o Ã© O(2^n), insustentÃ¡vel para n > 40
   - **SoluÃ§Ã£o:** Implementar versÃ£o iterativa O(n) ou memoizaÃ§Ã£o com cache
   - **Impacto Estimado:** ReduÃ§Ã£o de 80-90% em tempo de CPU por requisiÃ§Ã£o

2. **Aumento de Recursos de Pod**
   - **Problema:** Pods podem estar sub-provisionados (CPU/Memory limits baixos)
   - **SoluÃ§Ã£o:** Revisar `resources.requests` e `resources.limits` nos deployments
   - **SugestÃ£o:** Dobrar CPU/Memory limits como teste inicial

3. **Tuning de Timeout gRPC**
   - **Problema:** Timeouts padrÃ£o (30-60s) podem estar sendo atingidos
   - **SoluÃ§Ã£o:** Ajustar timeouts ou implementar circuit breaker
   - **ImplementaÃ§Ã£o:** Configurar deadline de gRPC para 10-15s e fail-fast

4. **Rate Limiting no Django**
   - **Problema:** Sistema aceita todas as requisiÃ§Ãµes, mesmo quando saturado
   - **SoluÃ§Ã£o:** Implementar rate limiting (ex: django-ratelimit) para proteger backend
   - **ConfiguraÃ§Ã£o:** Limitar a 40 req/s por IP ou globalmente

### OtimizaÃ§Ãµes Recomendadas (MÃ©dio Prazo)

1. **ImplementaÃ§Ã£o de Cache**
   - Usar Redis/Memcached para cachear resultados de Fibonacci
   - TTL de 1 hora para nÃºmeros frequentemente requisitados
   - Impacto: ReduÃ§Ã£o de 70-90% de carga no backend para valores repetidos

2. **RevisÃ£o de ConfiguraÃ§Ã£o HPA**
   - Reduzir threshold de CPU de 70% para 50%
   - Aumentar maxReplicas de 8 para 12-16
   - Configurar scale-down delay para evitar flapping

3. **Connection Pooling gRPC**
   - Verificar se Django mantÃ©m pool de conexÃµes gRPC persistentes
   - Configurar keep-alive para reduzir overhead de handshake

4. **Monitoramento Proativo**
   - Alertas quando P95 > 100ms ou taxa de erro > 1%
   - Dashboard com CPU/Memory por pod em tempo real
   - Trace distribuÃ­do (Jaeger/Zipkin) para identificar gargalos na cadeia Djangoâ†’Nodeâ†’Java

### Testes Futuros NecessÃ¡rios

1. **Teste de Busca de Limite**
   - Executar testes com 60, 70, 80, 90 usuÃ¡rios
   - Identificar exatamente onde ocorre degradaÃ§Ã£o
   - Objetivo: Mapear curva de capacidade

2. **Teste PÃ³s-OtimizaÃ§Ã£o**
   - ApÃ³s implementar otimizaÃ§Ãµes (Fibonacci iterativo, cache, etc.)
   - Re-executar Scenario 3 (100 usuÃ¡rios) e validar melhoria
   - Meta: Taxa de sucesso > 99%, P95 < 100ms

3. **Teste de Sustentabilidade**
   - Teste de 1 hora com carga sustentada em 80% da capacidade
   - Verificar memory leaks, degradaÃ§Ã£o gradual
   - Objetivo: Garantir estabilidade em operaÃ§Ã£o prolongada

4. **Teste de RecuperaÃ§Ã£o**
   - Simular falha de pod e observar tempo de recovery
   - Validar comportamento do HPA em spike sÃºbito de carga
   - Objetivo: Garantir resiliÃªncia a falhas

### PriorizaÃ§Ã£o de AÃ§Ãµes

| AÃ§Ã£o | Impacto | EsforÃ§o | Prioridade |
|------|---------|---------|-----------|
| Fibonacci iterativo/memoizaÃ§Ã£o | Alto | Baixo | ğŸ”´ P0 (CrÃ­tico) |
| Aumento de CPU/Memory limits | Alto | Baixo | ğŸ”´ P0 (CrÃ­tico) |
| Timeout gRPC tuning | MÃ©dio | Baixo | ğŸŸ¡ P1 (Alto) |
| Rate limiting Django | MÃ©dio | MÃ©dio | ğŸŸ¡ P1 (Alto) |
| Cache Redis | Alto | MÃ©dio | ğŸŸ¡ P1 (Alto) |
| HPA tuning | MÃ©dio | Baixo | ğŸŸ¢ P2 (MÃ©dio) |
| Monitoramento avanÃ§ado | Baixo | Alto | ğŸŸ¢ P2 (MÃ©dio) |

---

## ApÃªndice: DefiniÃ§Ãµes TÃ©cnicas

**Taxa de Falha:** Percentual de requisiÃ§Ãµes que resultaram em erro HTTP (neste caso, HTTP 500). Calculada como (Falhas / Total de RequisiÃ§Ãµes) Ã— 100.

**HTTP 500 (Internal Server Error):** Erro genÃ©rico do servidor indicando que a requisiÃ§Ã£o nÃ£o pÃ´de ser processada devido a problema interno (crash, exception, timeout, etc.).

**LatÃªncia P95:** 95Âº percentil de latÃªncia. Significa que 95% das requisiÃ§Ãµes completaram em tempo menor ou igual a este valor, e apenas 5% foram mais lentas.

**Tail Latency:** LatÃªncia dos percentis superiores (P95, P99, P99.9), representando os casos mais lentos. CrÃ­tica para experiÃªncia de usuÃ¡rio, pois mesmo que rara, impacta percepÃ§Ã£o geral.

**HPA (Horizontal Pod Autoscaler):** Recurso do Kubernetes que automaticamente aumenta ou diminui o nÃºmero de rÃ©plicas de pods baseado em mÃ©tricas (CPU, memÃ³ria, custom metrics).

**Throughput:** NÃºmero de requisiÃ§Ãµes processadas por segundo. NÃ£o diferencia entre requisiÃ§Ãµes bem-sucedidas e falhadas.

**gRPC (Google Remote Procedure Call):** Protocolo de comunicaÃ§Ã£o de alto desempenho usado entre os microserviÃ§os (Django â†’ Node.js â†’ Java).

**Circuit Breaker:** PadrÃ£o de design que previne cascading failures: se um serviÃ§o falha repetidamente, o circuit breaker "abre" e rejeita requisiÃ§Ãµes imediatamente sem tentar chamar o serviÃ§o degradado.

---

## Metadados do RelatÃ³rio

| Campo | Valor |
|-------|-------|
| VersÃ£o do RelatÃ³rio | 1.0 |
| Data de GeraÃ§Ã£o | 7 de Dezembro de 2025 |
| PerÃ­odo de Teste | 120 segundos |
| Ambiente | Kubernetes (Minikube) com HPA habilitado |
| Ferramenta de Teste | Locust |
| Status | âŒ CRÃTICO - Falha Operacional |
| AprovaÃ§Ã£o para ProduÃ§Ã£o | **NÃƒO APROVADO** |
| AÃ§Ãµes Bloqueantes | OtimizaÃ§Ã£o de algoritmo + Aumento de recursos |

---

**CLASSIFICAÃ‡ÃƒO FINAL: âŒ SISTEMA NÃƒO PRONTO PARA PRODUÃ‡ÃƒO COM 100+ USUÃRIOS**